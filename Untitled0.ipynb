{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNPDYsdiT71ozdFC7WOSIpU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sawharsha/msd/blob/main/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_QVhe6Efz87",
        "outputId": "4bbf3a33-e4b2-4080-8d41-5ad54d8168c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n",
            "Epoch 1/5\n",
            "750/750 [==============================] - 6s 6ms/step - loss: 0.3093 - accuracy: 0.9104 - val_loss: 0.1572 - val_accuracy: 0.9528\n",
            "Epoch 2/5\n",
            "750/750 [==============================] - 6s 8ms/step - loss: 0.1276 - accuracy: 0.9625 - val_loss: 0.1173 - val_accuracy: 0.9649\n",
            "Epoch 3/5\n",
            "750/750 [==============================] - 6s 7ms/step - loss: 0.0853 - accuracy: 0.9743 - val_loss: 0.1038 - val_accuracy: 0.9697\n",
            "Epoch 4/5\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.0652 - accuracy: 0.9805 - val_loss: 0.0877 - val_accuracy: 0.9735\n",
            "Epoch 5/5\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.0493 - accuracy: 0.9849 - val_loss: 0.0895 - val_accuracy: 0.9743\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.0890 - accuracy: 0.9737\n",
            "Test accuracy: 97.37%\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Load and preprocess the MNIST dataset\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "train_images = train_images.reshape((60000, 28, 28, 1)).astype('float32') / 255\n",
        "test_images = test_images.reshape((10000, 28, 28, 1)).astype('float32') / 255\n",
        "\n",
        "train_labels = to_categorical(train_labels)\n",
        "test_labels = to_categorical(test_labels)\n",
        "\n",
        "# Build the MLP model\n",
        "model = models.Sequential()\n",
        "model.add(layers.Flatten(input_shape=(28, 28, 1)))\n",
        "model.add(layers.Dense(128, activation='relu'))\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(train_images, train_labels, epochs=5, batch_size=64, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print(f'Test accuracy: {test_acc * 100:.2f}%')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Flatten, Dense\n",
        "\n",
        "# Load the IMDB dataset\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)\n",
        "\n",
        "# Preprocess the data by padding sequences\n",
        "max_len = 500  # Limit the length of each review to 500 words\n",
        "train_data = pad_sequences(train_data, maxlen=max_len)\n",
        "test_data = pad_sequences(test_data, maxlen=max_len)\n",
        "\n",
        "# Define the neural network model\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=10000, output_dim=32, input_length=max_len))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(16, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Display the model summary\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(train_data, train_labels, epochs=10, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_accuracy = model.evaluate(test_data, test_labels)\n",
        "print(f'Test Accuracy: {test_accuracy * 100:.2f}%')\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DubpL51DgDvs",
        "outputId": "dbb2e343-cac4-4409-866e-46f7b27b2515"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17464789/17464789 [==============================] - 0s 0us/step\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 500, 32)           320000    \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 16000)             0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 16)                256016    \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 576033 (2.20 MB)\n",
            "Trainable params: 576033 (2.20 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "625/625 [==============================] - 10s 15ms/step - loss: 0.4710 - accuracy: 0.7307 - val_loss: 0.3001 - val_accuracy: 0.8750\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 9s 15ms/step - loss: 0.1473 - accuracy: 0.9492 - val_loss: 0.3348 - val_accuracy: 0.8702\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 8s 13ms/step - loss: 0.0265 - accuracy: 0.9948 - val_loss: 0.3869 - val_accuracy: 0.8744\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 9s 15ms/step - loss: 0.0042 - accuracy: 0.9998 - val_loss: 0.4478 - val_accuracy: 0.8722\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 9s 14ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.4682 - val_accuracy: 0.8748\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 8s 13ms/step - loss: 5.5046e-04 - accuracy: 1.0000 - val_loss: 0.4966 - val_accuracy: 0.8760\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 11s 18ms/step - loss: 3.1364e-04 - accuracy: 1.0000 - val_loss: 0.5176 - val_accuracy: 0.8748\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 19s 30ms/step - loss: 1.9168e-04 - accuracy: 1.0000 - val_loss: 0.5409 - val_accuracy: 0.8762\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 9s 15ms/step - loss: 1.2293e-04 - accuracy: 1.0000 - val_loss: 0.5604 - val_accuracy: 0.8760\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 9s 15ms/step - loss: 8.0699e-05 - accuracy: 1.0000 - val_loss: 0.5807 - val_accuracy: 0.8760\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.5856 - accuracy: 0.8708\n",
            "Test Accuracy: 87.08%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.datasets import reuters\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# Load the Reuters dataset\n",
        "max_words = 10000\n",
        "(train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=max_words)\n",
        "\n",
        "# Convert the data to one-hot encoding\n",
        "def vectorize_sequences(sequences, dimension=max_words):\n",
        "    results = np.zeros((len(sequences), dimension))\n",
        "    for i, sequence in enumerate(sequences):\n",
        "        results[i, sequence] = 1.\n",
        "    return results\n",
        "\n",
        "x_train = vectorize_sequences(train_data)\n",
        "x_test = vectorize_sequences(test_data)\n",
        "\n",
        "# One-hot encode the labels\n",
        "num_classes = np.max(train_labels) + 1\n",
        "y_train = to_categorical(train_labels, num_classes)\n",
        "y_test = to_categorical(test_labels, num_classes)\n",
        "\n",
        "# Build the neural network model\n",
        "model = Sequential()\n",
        "model.add(Dense(64, activation='relu', input_shape=(max_words,)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "epochs = 10\n",
        "batch_size = 32\n",
        "history = model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_accuracy = model.evaluate(x_test, y_test)\n",
        "print(f'Test Accuracy: {test_accuracy * 100:.2f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Ncfs0c5gbaX",
        "outputId": "13297892-78d7-41d2-cb03-112b876d057d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/reuters.npz\n",
            "2110848/2110848 [==============================] - 0s 0us/step\n",
            "Epoch 1/10\n",
            "225/225 [==============================] - 4s 14ms/step - loss: 2.0904 - accuracy: 0.5170 - val_loss: 1.3605 - val_accuracy: 0.6928\n",
            "Epoch 2/10\n",
            "225/225 [==============================] - 3s 13ms/step - loss: 1.3923 - accuracy: 0.6656 - val_loss: 1.1885 - val_accuracy: 0.7195\n",
            "Epoch 3/10\n",
            "225/225 [==============================] - 4s 17ms/step - loss: 1.1877 - accuracy: 0.7129 - val_loss: 1.1179 - val_accuracy: 0.7373\n",
            "Epoch 4/10\n",
            "225/225 [==============================] - 3s 13ms/step - loss: 1.0218 - accuracy: 0.7435 - val_loss: 1.0720 - val_accuracy: 0.7596\n",
            "Epoch 5/10\n",
            "225/225 [==============================] - 3s 12ms/step - loss: 0.9133 - accuracy: 0.7730 - val_loss: 1.0461 - val_accuracy: 0.7724\n",
            "Epoch 6/10\n",
            "225/225 [==============================] - 3s 12ms/step - loss: 0.8385 - accuracy: 0.7823 - val_loss: 1.0681 - val_accuracy: 0.7785\n",
            "Epoch 7/10\n",
            "225/225 [==============================] - 4s 18ms/step - loss: 0.7853 - accuracy: 0.7932 - val_loss: 1.0641 - val_accuracy: 0.7846\n",
            "Epoch 8/10\n",
            "225/225 [==============================] - 3s 12ms/step - loss: 0.7204 - accuracy: 0.8141 - val_loss: 1.0626 - val_accuracy: 0.7885\n",
            "Epoch 9/10\n",
            "225/225 [==============================] - 3s 13ms/step - loss: 0.6716 - accuracy: 0.8221 - val_loss: 1.0919 - val_accuracy: 0.7835\n",
            "Epoch 10/10\n",
            "225/225 [==============================] - 3s 12ms/step - loss: 0.6238 - accuracy: 0.8312 - val_loss: 1.1160 - val_accuracy: 0.7841\n",
            "71/71 [==============================] - 0s 4ms/step - loss: 1.1972 - accuracy: 0.7756\n",
            "Test Accuracy: 77.56%\n"
          ]
        }
      ]
    }
  ]
}